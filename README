First and foremost, if you don't really really care,
you can safely ignore any and all licenses you see lying around.
If there are any issues whatsoever, drop me a line at
tomas@davidovic.cz and we will figure out whatever you might need.

With that out of the way, let's go over the usual stuff.

1) INSTALLATION and COMPILATION
TL;DR - Compiler smallvcm.cxx with OpenMP, if you don't have C++11 support,
define LEGACY_RNG (automatic for VS2008, found in rng.hxx).

The whole thing is one c++ source file and a lot of headers.
It was developed in VS2010. However, we did some limited testing on Linux (g++)
and the provide Makefile works for g++ 4.4 and 4.6.3 at least.

The major hurdle can come from the fact that C++11 <random> library
is used, but alternative random number generator is also provided.
The code expects OpenMP available, but getting rid of that is very
straightforward.

Other than that, it has no dependencies,
so just go ahead and compile smallvcm.cxx


2) FUNCTION
TL;DR - run smallvcm --report -t 10
In about 5-6 minutes it generates index.html, that
shows multiple algorithms on 4 different cornell boxes.

The program is set up to with four variants of Cornell Box:
	0    glossy small spheres + sun (directional)
	1    glossy large mirror sphere + ceiling (area)
	2    glossy small spheres + point
	3    glossy small spheres + background (env. lighting)
glossy ~ floor is glossy
small spheres ~ mirror and glass spheres in the box

It can run 7 different algorithms:
	el   eye light
	pt   path tracing
	lt   light tracing
	ppm  progressive photon mapping
	bpm  bidirectional photon mapping
	bpt  bidirectional path tracing
	vcm  vertex connection and merging

Aside from running individual combinations, switch --report can run all combinations,
while obeying required number of iterations and/or required maximum runtime for each combination.

3) SETTINGS  *TEMPORARY*
TL;DR - no setting is necessary, out of the box generates 512x512 image with Vertex Connection Merging.
Also, try --report -t 10 for all algorithm-scene combinations running 10s each.

Scene, algorithm, number of iterations, runtime and output filename can be set from commandline.
Use -h, --help or /? to access help.

All default settings are set in ParseCommandline function in config.hxx.
The settings that have no commandline switch and must be changed in the code are:
	mNumThreads ~ Number of rendering threads (default 0, means 1 thread per processor)
	mBaseSeed   ~ Seed for random number generators (default 1234)
	mMaxPathLength ~ Maximal number of path segments (default 10)
	mMinPathLength ~ Minimal number of path segments (default 0)
	mResolution    ~ Resolution of final image (default 512x512)

4) SUPPORTED FEATURES and LIMITATIONS
Originally this was intended to be very short example, akin to SimplePT.
However, it kinda bloated over time. Here is list of features and limitations.
Limitations:
a) No acceleration structure is used (can be added to scene.hxx)
b) Scenes are hardcoded (see LoadCornellBox in scene.hxx)
c) Materials are limited (diffuse, phong, mirror, glass, not all combinations)
d) Each area light is an individual triangle, and requires its very own material for identification.
	This can and should be changed if code is modified for any more complex scenes.
e) No subsurface scattering, no participating media
f) No shading normals

Features:
a) All basic types of light (area, point, directional, and env. map) are
	presented, even though env. map is very simple constant radiance.
	The way of finding whether and which area light we hit by randomly
	tracing ray should be revisited before using this for anything serious.
	(see Limitation d)
b) Basic materials are supported, and should be fairly straightforward
	to extend to more materials. I would suggest changing the actual
	implementation, but the interfaces should suffice for most.
c) Material instances are represented by BRDF object.
	This stores the incoming (fixed) direction and answers questions
	(sample, evaluate brdf, evaluate pdf) always w.r.t. this direction.
	Also stores all the other important things, like local frame.
	Extending the framework by shading normals should happen within
	this BRDF object (has template parameter ~ Veach's Adjoint brdf).
d) For fast preview supports Eye Light shader (color ~ Dot(L, N)).
	Answers in red instead of gray when back is hit.
	eyelight.hxx
e) Standard path tracing with next even estimation (both area and env. map)
	pathtracer.hxx
f) All the other algorithms, i.e, Light Tracer (lt), Progressive Photon Mapping (ppm),
	Bidirectional Photon Mapping (bpm), Bidirectional Path Tracing (bpt), and ours
	Vertex Connection and Merging (vcm), are in vertexcm.hxx.
	They all almost identical code paths, with the chief differences being
	in the weights management (with occasional shortcut through unused code).
	These are described in the next section

5) ALGORITHM(s)
Here I will try to describe what happens in VertexCM on higher level,
and how each algorithm utilises this.

The basic consists of three parts:
	Light particle tracing - photons, light paths for bpt/vcm
	Hash grid build - for range search (ppm, bpm, vcm)
	Camera particle tracing - for all but light tracing

Particle (also called PathElement, or LightSample/CameraSample) is our basic
structure describing current sample. The only unusual members are: d0, d1vc, d1vm.
These are used for Multiple Importance Sampling (where required):
	d0 - used for both connections (bpt, vcm) and merging (bpm, vcm)
	d1vc - used for connections (bpt, vcm)
	d1vm - used for merging (bpm, vcm)

a) Light Tracing
	Light Tracing utilises only Light particle tracing.
	Each particle is directly connected to camera, not stored.
	No MIS is used, nor is hash grid or camera tracing.

b) Progressive Photon Mapping
	Light particles are traced (do not contribute to camera).
	When on non-delta BSDF, they are stored (as LightVertex).
	Hash grid is built.
	Camera particles are traced until first non-specular bounce is hit.
	Then merging (photon lookup) is performed and camera path is terminated.
	(This means we cannot handle specular + non-specular mix,
	there is an advice on how to extend this in the code).
	No MIS is used, camera paths can return radiance from directly hit lights,
	when all bounces on the path are specular.

c) Bidirectional Photon Mapping
	Almost identical to PPM, except that camera path does not terminate after
	first merging. MIS is used (d0, and d1vm) to weight the separate mergings
	along the camera path.

b) Bidirection Path Tracing
	Light particles are traced (do contribute to camera).
	They are stored (as LightVertex).
	We keep track of which path has which particles.
	Hash grid is not built.
	Camera particles are traced. On each bounce, camera vertex is
	connected to all legal light vertices from light path with same number.
	MIS is used (d0, d1vc)

e) Vertex Connection and Merging
	Everything is used. Light particles traced, stored, hash grid is built, etc.
	MIS is used (d0, d1vm, d1vc).

6) DISCLAIMER
I am well aware that the description here is not complete and can be somewhat
confusing. If you have any questions and/or input w.r.t. improving and adding
explanations, feel free to contact me at tomas@davidovic.cz.
