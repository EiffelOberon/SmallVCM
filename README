First and foremost, if you don't really really care,
you can safely ignore any and all licenses you see lying around.
If there are any issues whatsoever, drop me a line at
tomas@davidovic.cz and we will figure out whatever you might need.

With that out of the way, let's go over the usual stuff.

1) INSTALLATION and COMPILATION
The whole thing is one c++ source file and a lot of headers.
It was developed in VS2010. However, we did some limited testing on Linux (g++)
and the provided Makefile works for g++ 4.4 and 4.6.3 at least.

The major hurdle can come from the fact that C++11 <random> library
is used, but alternative random number generator is also provided.
The code expects OpenMP available, but getting rid of that is very
straightforward.

Other than that, it has no dependencies,
so just go ahead and compile smallvcm.cxx


2) FUNCTION
The program is set up to iterate through several variants of Cornell Box:
Empty, 2 small spheres (mirror and glass), 1 large mirror sphere.
And four variants of lighting (area, point, direction, and env. map lighting).

Then runs Path Tracing, Light Tracing, Progressive Photon Mapping,
Bidirectional Photon Mapping, Bidirectional Path Tracing, and finally
Vertex Connection and Merging algorithms on each.

It produces report.html file that sums presents all the produced images.
This is not properly formatted html but at least Firefox, Chrome,
and Internet Explorer can handle it, even when program is still running.

On 4 core machine, it takes about 2 seconds per image (with 10 iterations each).
You can have a look at 100 iterations each version in results_100 directory.

3) SETTINGS  *TEMPORARY*
All settings in this section are to be found in smallvcm.cxx, main.
The program currently does supports a single parameter,
which is number of iterations. The default is 10.
On 4 core machine, 10 iterations take about 2-5 seconds
(depending on the algorithm).

The other two important parameters, that have to be changed in smallvcm.cxx,
are resolution (256x256) and maximum path length (10 segments).

There are 2 variants on the cornell box, one has diffuse floor and white back wall,
the other has glossy floor and blue back wall. The default renders both, but you can
use bools glossy and diffuse to turn on/off either.

sceneConfigs sets which scenes are used, algorithmMask sets which algorithms
will be run. Thumbnail size sets size of thumbnails in report.html

4) SUPPORTED FEATURES and LIMITATIONS
Originally this was intended to be very short example, akin to SimplePT.
However, it kinda bloated over time. Here is list of features and limitations.
Limitations:
a) No acceleration structure is used (can be added to scene.hxx)
b) Scenes are hardcoded (see LoadCornellBox in scene.hxx)
c) Materials are limited (diffuse, phong, mirror, glass, not all combinations)
d) No subsurface scattering, no participating media (still Work In Progress)
e) No shading normals

Features:
a) All basic types of light (area, point, directional, and env. map) are
	presented, even though env. map is very simple constant radiance.
	The way of finding whether and which area light we hit by randomly
	tracing ray should be revisited before using this for anything serious.
b) Basic materials are supported, and should be fairly straightforward
	to extend to more materials. I would suggest changing the actual
	implementation, but the interfaces should suffice for most.
c) Material instances are represented by BXDF object.
	This stores the incoming (fixed) direction and answers questions
	(sample, evaluate brdf, evaluate pdf) always w.r.t. this direction.
	Also stores all the other important things, like local frame.
	Extending the framework by shading normals should happen within
	this BXDF object (has template parameter ~ Veach's Adjoint brdf).
d) For fast preview supports Eye Light shader (color ~ Dot(L, N)).
	Answers in red instead of gray when back is hit.
	eyelight.hxx
e) Standard path tracing with next even estimation (both area and env. map)
	pathtracer.hxx
f) All the other algorithms, i.e, Light Tracer (lt), Progressive Photon Mapping (ppm),
	Bidirectional Photon Mapping (bpm), Bidirectional Path Tracing (bpt), and ours
	Vertex Connection and Merging (vcm), are in vertexcm.hxx.
	They all almost identical code paths, with the chief differences being
	in the weights management (with occasional shortcut through unused code).
	These are described in the next section

5) ALGORITHM(s)
Here I will try to describe what happens in VertexCM on higher level,
and how each algorithm utilises this.

The basic consists of three parts:
	Light particle tracing - photons, light paths for bpt/vcm
	Hash grid build - for range search (ppm, bpm, vcm)
	Camera particle tracing - for all but light tracing

Particle (also called PathElement, or LightSample/CameraSample) is our basic
structure describing current sample. The only unusual members are: d0, d1vc, d1vm.
These are used for Multiple Importance Sampling (where required):
	d0 - used for both connections (bpt, vcm) and merging (bpm, vcm)
	d1vc - used for connections (bpt, vcm)
	d1vm - used for merging (bpm, vcm)

a) Light Tracing
	Light Tracing utilises only Light particle tracing.
	Each particle is directly connected to camera, not stored.
	No MIS is used, nor is hash grid or camera tracing.

b) Progressive Photon Mapping
	Light particles are traced (do not contribute to camera).
	When on non-delta BXDF, they are stored (as LightVertex).
	Hash grid is built.
	Camera particles are traced until first non-specular bounce is hit.
	Then merging (photon lookup) is performed and camera path is terminated.
	(This means we cannot handle specular + non-specular mix,
	there is an advice on how to extend this in the code).
	No MIS is used, camera paths can return radiance from directly hit lights,
	when all bounces on the path are specular.

c) Bidirectional Photon Mapping
	Almost identical to PPM, except that camera path does not terminate after
	first merging. MIS is used (d0, and d1vm) to weight the separate mergings
	along the camera path.

b) Bidirection Path Tracing
	Light particles are traced (do contribute to camera).
	They are stored (as LightVertex).
	We keep track of which path has which particles.
	Hash grid is not built.
	Camera particles are traced. On each bounce, camera vertex is
	connected to all legal light vertices from light path with same number.
	MIS is used (d0, d1vc)

e) Vertex Connection and Merging
	Everything is used. Light particles traced, stored, hash grid is built, etc.
	MIS is used (d0, d1vm, d1vc).

6) DISCLAIMED
I am well aware that the description here is not complete and can be somewhat
confusing. If you have any questions and/or input w.r.t. improving and adding
explanations, feel free to contact me at tomas@davidovic.cz.
